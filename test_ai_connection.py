#!/usr/bin/env python3
"""
Script para probar la conexiÃ³n con LM Studio
"""
import requests
import json

# ConfiguraciÃ³n
API_URL = "http://localhost:1234/v1/chat/completions"
MODEL = "meta-llama-3-8b-instruct"

def test_lm_studio_connection():
    """Prueba la conexiÃ³n con LM Studio"""
    
    print("ğŸ” Probando conexiÃ³n con LM Studio...")
    print(f"ğŸŒ URL: {API_URL}")
    print(f"ğŸ¤– Modelo: {MODEL}")
    print("-" * 50)
    
    # Test 1: Verificar que el servidor estÃ© ejecutÃ¡ndose
    try:
        models_url = "http://localhost:1234/v1/models"
        response = requests.get(models_url, timeout=5)
        
        if response.status_code == 200:
            print("âœ… Servidor LM Studio: FUNCIONANDO")
            models = response.json()
            available_models = [model['id'] for model in models['data']]
            print(f"ğŸ“‹ Modelos disponibles: {available_models}")
            
            if MODEL in available_models:
                print(f"âœ… Modelo '{MODEL}': ENCONTRADO")
            else:
                print(f"âŒ Modelo '{MODEL}': NO ENCONTRADO")
                print("ğŸ’¡ Modelos disponibles:", available_models)
                return False
        else:
            print(f"âŒ Error HTTP: {response.status_code}")
            return False
            
    except requests.exceptions.ConnectionError:
        print("âŒ Error: No se puede conectar a LM Studio")
        print("ğŸ’¡ AsegÃºrate de que LM Studio estÃ© ejecutÃ¡ndose en localhost:1234")
        return False
    except requests.exceptions.Timeout:
        print("âŒ Error: Timeout de conexiÃ³n")
        return False
    except Exception as e:
        print(f"âŒ Error inesperado: {e}")
        return False
    
    # Test 2: Probar generaciÃ³n de texto
    print("\nğŸ§ª Probando generaciÃ³n de texto...")
    
    try:
        payload = {
            "model": MODEL,
            "messages": [
                {"role": "system", "content": "Eres un asistente Ãºtil que responde en espaÃ±ol."},
                {"role": "user", "content": "Responde brevemente: Â¿EstÃ¡s funcionando correctamente?"}
            ],
            "temperature": 0.3,
            "max_tokens": 100,
            "stream": False
        }
        
        response = requests.post(API_URL, json=payload, timeout=30)
        
        if response.status_code == 200:
            data = response.json()
            message = data['choices'][0]['message']['content'].strip()
            print("âœ… GeneraciÃ³n de texto: FUNCIONANDO")
            print(f"ğŸ¤– Respuesta del modelo: '{message}'")
            return True
        else:
            print(f"âŒ Error en generaciÃ³n: HTTP {response.status_code}")
            print(f"ğŸ“„ Respuesta: {response.text}")
            return False
            
    except requests.exceptions.Timeout:
        print("âŒ Error: El modelo tardÃ³ demasiado en responder")
        print("ğŸ’¡ El modelo puede estar cargÃ¡ndose. Intenta de nuevo en unos minutos.")
        return False
    except Exception as e:
        print(f"âŒ Error en generaciÃ³n: {e}")
        return False

def show_troubleshooting():
    """Muestra guÃ­a de soluciÃ³n de problemas"""
    print("\n" + "="*60)
    print("ğŸ› ï¸  GUÃA DE SOLUCIÃ“N DE PROBLEMAS")
    print("="*60)
    print()
    print("Si el test fallÃ³, verifica lo siguiente:")
    print()
    print("1. ğŸ“± LM Studio estÃ¡ instalado y abierto")
    print("   - Descargar desde: https://lmstudio.ai/")
    print()
    print("2. ğŸ¤– El modelo estÃ¡ descargado")
    print("   - Ir a 'Discover' en LM Studio")
    print("   - Buscar: 'meta-llama-3-8b-instruct'")
    print("   - Descargar el modelo")
    print()
    print("3. ğŸŒ El servidor estÃ¡ iniciado")
    print("   - Ir a 'Local Server' en LM Studio")
    print("   - Seleccionar el modelo")
    print("   - Hacer clic en 'Start Server'")
    print("   - Verificar: 'Server running on http://localhost:1234'")
    print()
    print("4. ğŸ”¥ El firewall no estÃ¡ bloqueando")
    print("   - Permitir conexiones a localhost:1234")
    print()
    print("5. ğŸ’¾ Suficiente memoria RAM")
    print("   - Llama-3-8B requiere ~8GB de RAM")
    print("   - Considera un modelo mÃ¡s pequeÃ±o si tienes menos RAM")

if __name__ == "__main__":
    print("ğŸš€ PRUEBA DE CONEXIÃ“N CON LM STUDIO")
    print("="*50)
    
    success = test_lm_studio_connection()
    
    if success:
        print("\nğŸ‰ Â¡TODO FUNCIONANDO CORRECTAMENTE!")
        print("ğŸ’¡ Tu aplicaciÃ³n de anÃ¡lisis de datos puede usar IA ahora.")
    else:
        print("\nâŒ HAY PROBLEMAS DE CONEXIÃ“N")
        show_troubleshooting()